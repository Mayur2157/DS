{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f59951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a84c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      " I am Mrunal.\n",
      "\n",
      "Tokens:\n",
      " ['I', 'am', 'Mrunal', '.']\n",
      "\n",
      "POS Tags:\n",
      " [('I', 'PRP'), ('am', 'VBP'), ('Mrunal', 'NNP'), ('.', '.')]\n",
      "\n",
      "Filtered Tokens (after stop words removal):\n",
      " ['Mrunal', '.']\n",
      "\n",
      "Stemmed Tokens:\n",
      " ['mrunal', '.']\n",
      "\n",
      "Lemmatized Tokens:\n",
      " ['Mrunal', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mayur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mayur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mayur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mayur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the document variable\n",
    "document = \"I am Mrunal.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Document:\\n\", document)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "print(\"\\nPOS Tags:\\n\", pos_tags)\n",
    "print(\"\\nFiltered Tokens (after stop words removal):\\n\", filtered_tokens)\n",
    "print(\"\\nStemmed Tokens:\\n\", stemmed_tokens)\n",
    "print(\"\\nLemmatized Tokens:\\n\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d400eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 nd part'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"2 nd part\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d252f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "artificial: 0.3817\n",
      "intelligence: 0.3817\n",
      "is: 0.3009\n",
      "language: 0.3009\n",
      "natural: 0.3009\n",
      "of: 0.3817\n",
      "processing: 0.3817\n",
      "subfield: 0.3817\n",
      "\n",
      "Document 2:\n",
      "and: 0.2392\n",
      "between: 0.3034\n",
      "computers: 0.3034\n",
      "focuses: 0.3034\n",
      "humans: 0.3034\n",
      "interaction: 0.3034\n",
      "it: 0.3034\n",
      "language: 0.2392\n",
      "natural: 0.2392\n",
      "on: 0.3034\n",
      "the: 0.3034\n",
      "using: 0.3034\n",
      "\n",
      "Document 3:\n",
      "analysis: 0.2686\n",
      "and: 0.2117\n",
      "applications: 0.2686\n",
      "are: 0.2686\n",
      "as: 0.2686\n",
      "in: 0.2117\n",
      "machine: 0.2686\n",
      "nlp: 0.2117\n",
      "sentiment: 0.2686\n",
      "such: 0.2686\n",
      "techniques: 0.2686\n",
      "translation: 0.2686\n",
      "used: 0.2686\n",
      "various: 0.2686\n",
      "widely: 0.2686\n",
      "\n",
      "Document 4:\n",
      "an: 0.4129\n",
      "essential: 0.4129\n",
      "in: 0.3256\n",
      "is: 0.3256\n",
      "nlp: 0.3256\n",
      "preprocessing: 0.4129\n",
      "step: 0.4129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of documents\n",
    "documents = [\n",
    "    \"Natural language processing is a subfield of artificial intelligence.\",\n",
    "    \"It focuses on the interaction between computers and humans using natural language.\",\n",
    "    \"NLP techniques are widely used in various applications such as sentiment analysis and machine translation.\",\n",
    "    \"Preprocessing is an essential step in NLP.\",\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    for j, term in enumerate(feature_names):\n",
    "        tfidf_value = tfidf_matrix[i, j]\n",
    "        if tfidf_value > 0:\n",
    "            print(f\"{term}: {tfidf_value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fe1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
